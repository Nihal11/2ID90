\documentclass[11pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage[linesnumbered,noend]{algorithm2e}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{ % Copied from: http://tex.stackexchange.com/a/847/41003
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% New paragraph = blank line, not indent.
\setlength{\parskip}{0.3cm}
\setlength{\parindent}{0pt}

% Code listings
\usepackage{listings}
\usepackage{color,courier}

\definecolor{dkgreen}{rgb}{0,0.55,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{language=Python,
linewidth=80em,
breaklines=true,
numbers=left,
basicstyle=\ttfamily\footnotesize,
showstringspaces=false,
numberstyle=\tiny\color{gray},
keywordstyle=\color{blue},
commentstyle=\color{dkgreen},
stringstyle=\color{mauve},
}


\title{Spelling Correction}
\author{
Dennis van der Schagt (0814249)\\
Rob Wu (0787817)
}
\date{\today}


\begin{document}
\maketitle
\newpage
\section*{Abstract}


\section{The algorithm}
To find the best suggestion for a spelling correction, potential spelling corrections need to be generated and evaluated.

We assume that the input text contains at most two misspelled words, which are not next to each other.
To find the most appropriate spelling correction, we first rate the original text, and then re-evaluate the text with one or two words modified.
We apply the noisy channel model and assume that a word contains only one typographical mistake (insertion, deletion or substitution of one character, or transposition of two consecutive characters). For each input word, a list of such corrections is generated. These corrections are called \textit{candidates}. The output where words may be replaced by candidates is called a \textit{suggestion}.

To evaluate a suggestion, the likelihood of a candidate is used, as well as the likelihood of the bigrams of candidates within the suggestion.
These likelihoods are combined (the exact method is explained later) and form the \textit{combined likelihood}.
After calculating the likelihoods for every suggestion, the one with the maximal \textit{combined likelihood} is presented as the final suggestion for spelling correction.

\subsection{General flow}
A naive way to implement the previous concepts is to generate a list containing every possible suggestion, and then iterate over the list to find the maximal value.
Although this method is simple, it is not the best choice because it does not scale well: the number of suggestions is of the order $O(\textit{[number of candidates per word]}^2 \times \textit{[number of words in the input text]})$.
This excessive memory demand can be solved by immediately evaluating the suggestion when it is generated, and replacing the then-best suggestion with the just-evaluated suggestion when its combined likelihood is higher.

Suggestions are generated as follows. First, we split the input text into a list of words. Then, for each word, we generate and store a list of candidates (as elaborated in section \ref{generateCandidateWords}). This step requires $O(\textit{[number of candidates per word]} \times \textit{[number of words in the input text]})$ memory. If peak memory usage is a concern, this step can be omitted at the cost of speed (time) in the next step.

The actual suggestion generation is done by using the assumption that at most two non-consecutive errors occur in the input text. For every word, we iterate over the list of candidates and evaluate the suggestion that is equal to the original text, but with the given word replaced by a candidate. Similar logic is used to find and evaluate the second spelling error. The original word is also rated, since it is also possible that the input is correct, i.e. contains zero spelling errors.

The pseudo-code of the general structure is displayed below.

\begin{lstlisting}[language=python]
words = input.split(' ')
candidates = []
for word in words:
   # getCandidateWords generates a set of candidates for each word; its
   # exact implementation is explained later.
   candidates.append(getCandidateWords(word))

# The program will not output a suggestion if a word is not in the dictionary.
bestSuggestion = input if input in dictionary  else  ""

for error1 in range( 0, len(words) ):
    for candidate1 in candidates:
        # Evaluate the suggestion, with the word at position |error1| replaced
        # by |candidate1|.
        evaluate(error1, candidate1)
      
        # Try to find the next error. The next word after a corrected word is
        # assumed to be correct, so skip the word at the next position and
        # start iterating over the words from the next position, i.e. at
        # position |error1| + 2.
        for error2 in range( error1 + 2, len(words) ):

            for candidate2 in candidates:
                # Evaluate the suggestion, with the word at position |error2|
                # replaced with |candidate2|. Note that the word at position
                # |error1| is still |candidate1|.
                evaluate(error2, candidate2)
        
         # restore the original word at position |error2|, so that the next
         # iteration over |error1| will use the original word.
         restore(error2)

     # restore the original word at position |error1|
     restore(error1)

# At this point, every possible combination of candidates in the suggestion has
# been checked by the evaluate function, and the best suggestion has been
# selected. Show this suggestion.
print(bestSuggestion)
\end{lstlisting}

\subsection{Generating candidates}\label{generateCandidateWords}
As said before, we assume that a candidate is a word in the dictionary, and differs from the original word by at most one modification, which is either a character insertion/deletion/substitution or a transposition of consecutive characters.
Each of these mutations are defined in terms of a range (start + end position) and replacement.
A precise definition and explanation of the parameters for these operations (given a word) is shown in the pseudo-code below.

\begin{lstlisting}
# Insert space (word separator) around word to simplify the implementation of
# the insertion/deletion logic (without these, we have to add and handle extra
# bound checks for the first and last character).
word = ' ' + word + ' '

# Insertion
# Start at 1 because insertion looks at the previous letter. It is safe to skip
# the first character because we have prepended a space before the word.
for i in range(1, len(word)):
    for newLetter in ALPHABET:
        collect(i - 1, i, word[i - 1] + newLetter)

# Deletion
# Exclude the first and last characters (spaces) because the body looks at the
# previous and next letter.
for i in range(1, len(word) - 1):
    collect(i - 1, i + 1, word[i - 1])

# Transposition
# Exclude the first character, because we do not want to break the word in two
# words (this would happen when the leading space is swapped with the second
# character). The last character is skipped for the same reason, and the
# 
for i in range(1, len(word) - 2):
    collect(i, i + 2, word[i + 1] + word[i])

# Substitution
# Skip the first and last character (space) because replacing them with letters
# would be equivalent to adding letters before/after a word. This is already
# covered by the insertion logic.
for i in range(1, len(word) - 1):
    for newLetter in ALPHABET:
        collect(i, i + 1, newLetter)
\end{lstlisting}

In the pseudo-code, the \texttt{collect} function is invoked several times.
This method generates a potential candidate (by replacing the characters in the given range by the given replacement) and checks whether the word is in the dictionary.
If the word exists, the probability that the edit is correct is calculated and stored together with the candidate (this probability is used many times during the evaluation of suggestions, so it makes sense to calculate and store it in advance).
This probability is also known as "channel model probability" within the noisy channel framework, and is a product of two factors: the \textit{prior} and the \textit{edit probability}.
The prior is simply the frequency of the word, and the edit probability is the smoothened edit frequency. This is derived from the confusion matrix from Kernighan's paper \cite{ref:confusion}.

Some candidates can be generated in multiple ways, e.g. "ass" can become "as" by removing any of the two "s" characters.
This possibility is accounted for by summing the calculated probabilities (up to a maximum of 1).

\subsection{Bigram language model probability}
\subsection{Generating suggestions}

\section{Results}
The spell checker can be configured by 4 parameters which can be changed by altering the constants directly in the code. The quality of the spell checker strongly depends on those parameters. Optimal values for those parameters were experimentally determined by running the spell checker with our training data as input. We used Peach's test cases as test data. Of those tests, the spell checker passes 30 out of 33.

The 4 parameters are called: probability-no-edit-needed, edit-probability-k-smoothing, bigram-smoothing, and add-k-prior. Following is a brief description of those parameters and their influence on the output.
\begin{itemize}
\item probability-no-edit-needed: Every word with a Damerau-Levenshtein distance of 1 to an original word is a spell correction candidate. To all of those words we assign an edit probability which depends on the specific alteration in combination with the confusion matrix. To be able to compare candidate probabilities with the probability of the original word, we have to assign an edit probability to the original word. We choose a value of 0.95 which means that we assume that 1 out of 200 words contains a spelling error. By keeping this probability this high we make sure that only words that are significantly better will replace the original word.

\item edit-probability-k-smoothing: The confusion matrix contains all probable alterations that correct a word. Nevertheless, words sometimes contain spelling errors that are not in the confusion matrix. Initially we just didn't correct those errors but we discovered that by simply adding a constant $k$ to all results the confusion matrix got smoothed enough to solve those errors. We now use a save value of $1$ for this instance of add-k smoothing. Increasing this value adds more smoothing but it also lowers the influence of the actual confusion matrix.

\item bigram-smoothing: Many 2-word combinations of the correct sentences do not occur as bigrams in the N-gram input file. This was the cause of many sentences (in which the individual words had a high probability and were actually correct) to get discarded because of the bigrams of the words not being present in the N-gram data. To fix this, smoothing of the bigram data is needed. We first smoothed by adding $1$ to every bigram count. This turned out to be too high. In the end we settled with add-$0.01$ smoothing. This makes sure that the bigram information stays relevant while still allowing new bigrams to be included in the suggested sentences.

To show the problem with add-1 smoothing, take the bigrams "is present" and "us present", which have respective counts $7$ and $0$. The unigrams "is" and "us" have respective counts $12157$ and $628$. We calculate a bigram's probabilility by dividing the smoothed bigram count by the smoothed count of the left word. This would assign a probability of $8/12158$ to "is present" and a $1/628$ probability to "us present". Here "us present" clearly has a much higher probability while, intuitively, "is present" should have a higher probability. Add-$0.01$ smoothing clearly performs better here with probabilities of $7.01/12157.01$ and $0.01/628.01$ for "is present" and "us present" respectively.
\item add-k-prior: The add-k-prior parameter regulates the smoothing for unigrams. We have used a value of $0.5$ and applied it as described by Kernighan et al. (1990) but as we are already ignoring words outside of the vocabulary this smoothing does not add any value. If we were ever to allow unknown words in suggestions this smoothing should be enabled to increase the probability for these words.
\end{itemize}

\section{Reproducing the results}
Following are the steps required to reproduce our results:
\begin{itemize}
\item Download the Netbeans project of our Spelling correction program from Peach.
\item Compile the project, e.g. using \textsc{ant compile}.
\item Run the project, e.g. using \textsc{ant run}.
\item Type a sentence consisting of lowercase Latin letters and spaces, with at most two spelling errors and press enter.
\item Read the result. The corrected sentence will be displayed as "Answer: [corrected sentence].
\end{itemize}

\section{References}


\section{Contributions}
Dennis and Rob contributed equally to this assignment.


\end{document}














